{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDNs",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-amxgynvRPu",
        "outputId": "2085ca2d-bad1-4a21-f982-5fbb1029b88d"
      },
      "source": [
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version 1.7.0+cu101\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvaGchoeyNKu",
        "outputId": "b47dca0e-a802-436e-fb51-4884632d9159"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2-hbkrZyKHX"
      },
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/Final Project/'\n",
        "os.chdir(BASE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gsq2EsVzUGs"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftbmdy8rvl4-"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as f\n",
        "from torch.distributions.normal import Normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knLYNW0TyZNb",
        "outputId": "570d6fc0-8a4e-43e8-ccf2-fc1ed4c5f1ab"
      },
      "source": [
        "USE_CUDA = True\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "NUM_WORKERS = multiprocessing.cpu_count()\n",
        "print('num workers:', NUM_WORKERS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcrJa9SsvpL6"
      },
      "source": [
        "def gmm_loss(batch, mus, sigmas, logpi, reduce=True): # pylint: disable=too-many-arguments\n",
        "    \"\"\" Computes the Gaussian Mixture Model (GMM) loss.\n",
        "    Compute minus the log probability of batch under the GMM model described\n",
        "    by mus, sigmas, pi. Precisely, with bs1, bs2, ... the sizes of the batch\n",
        "    dimensions (several batch dimension are useful when you have both a batch\n",
        "    axis and a time step axis), gs the number of mixtures and fs the number of\n",
        "    features.\n",
        "    :args batch: (bs1, bs2, *, fs) torch tensor\n",
        "    :args mus: (bs1, bs2, *, gs, fs) torch tensor\n",
        "    :args sigmas: (bs1, bs2, *, gs, fs) torch tensor\n",
        "    :args logpi: (bs1, bs2, *, gs) torch tensor\n",
        "    :args reduce: if not reduce, the mean in the following formula is ommited\n",
        "    :returns:\n",
        "    loss(batch) = - mean_{i1=0..bs1, i2=0..bs2, ...} log(\n",
        "        sum_{k=1..gs} pi[i1, i2, ..., k] * N(\n",
        "            batch[i1, i2, ..., :] | mus[i1, i2, ..., k, :], sigmas[i1, i2, ..., k, :]))\n",
        "    NOTE: The loss is not reduced along the feature dimension (i.e. it should scale ~linearily\n",
        "    with fs).\n",
        "    \"\"\"\n",
        "    batch = batch.unsqueeze(-2)\n",
        "    normal_dist = Normal(mus, sigmas)\n",
        "    g_log_probs = normal_dist.log_prob(batch)\n",
        "    g_log_probs = logpi + torch.sum(g_log_probs, dim=-1)\n",
        "    max_log_probs = torch.max(g_log_probs, dim=-1, keepdim=True)[0]\n",
        "    g_log_probs = g_log_probs - max_log_probs\n",
        "\n",
        "    g_probs = torch.exp(g_log_probs)\n",
        "    probs = torch.sum(g_probs, dim=-1)\n",
        "\n",
        "    log_prob = max_log_probs.squeeze() + torch.log(probs)\n",
        "    if reduce:\n",
        "        return - torch.mean(log_prob)\n",
        "    return - log_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_NZr4fr7Wan"
      },
      "source": [
        "def total_loss(latent_obs, action, reward, terminal, latent_next_obs, model):\n",
        "  latent_obs, action,\\\n",
        "        reward, terminal,\\\n",
        "        latent_next_obs = [arr.transpose(1, 0)\n",
        "                           for arr in [latent_obs, action,\n",
        "                                       reward, terminal,\n",
        "                                       latent_next_obs]]\n",
        "  mus, sigmas, logpi, rs, ds = model(action, latent_obs)\n",
        "  gmm = gmm_loss(latent_next_obs, mus, sigmas, logpi)\n",
        "  bce = f.binary_cross_entropy_with_logits(ds, terminal)\n",
        "  mse = f.mse_loss(rs, reward)\n",
        "  scale = 32 + 2 # LATENT_SIZE=32 plus other losses\n",
        "  return (gmm + bce + mse) / scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN_qW6zXv1zL"
      },
      "source": [
        "class _MDRNNBase(nn.Module):\n",
        "    def __init__(self, latents, actions, hiddens, gaussians):\n",
        "        super(_MDRNNBase, self).__init__()\n",
        "        self.latents = latents\n",
        "        self.actions = actions\n",
        "        self.hiddens = hiddens\n",
        "        self.gaussians = gaussians\n",
        "\n",
        "        self.gmm_linear = nn.Linear(\n",
        "            hiddens, (2 * latents + 1) * gaussians + 2)\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd8Q8fDRtgyf"
      },
      "source": [
        "class MDRNN(_MDRNNBase):\n",
        "    \"\"\" MDRNN model for multi steps forward \"\"\"\n",
        "    def __init__(self, latents, actions, hiddens, gaussians):\n",
        "        super(MDRNN, self).__init__(latents, actions, hiddens, gaussians)\n",
        "        self.rnn = nn.LSTM(latents + actions, hiddens)\n",
        "\n",
        "    def forward(self, actions, latents): # pylint: disable=arguments-differ\n",
        "        \"\"\" MULTI STEPS forward.\n",
        "        :args actions: (SEQ_LEN, BSIZE, ASIZE) torch tensor\n",
        "        :args latents: (SEQ_LEN, BSIZE, LSIZE) torch tensor\n",
        "        :returns: mu_nlat, sig_nlat, pi_nlat, rs, ds, parameters of the GMM\n",
        "        prediction for the next latent -> rs, gaussian prediction of the reward and\n",
        "        logit prediction of terminality -> ds.\n",
        "            - mu_nlat: (SEQ_LEN, BSIZE, N_GAUSS, LSIZE) torch tensor\n",
        "            - sigma_nlat: (SEQ_LEN, BSIZE, N_GAUSS, LSIZE) torch tensor\n",
        "            - logpi_nlat: (SEQ_LEN, BSIZE, N_GAUSS) torch tensor\n",
        "            - rs: (SEQ_LEN, BSIZE) torch tensor\n",
        "            - ds: (SEQ_LEN, BSIZE) torch tensor\n",
        "        \"\"\"\n",
        "        seq_len, bs = actions.size(0), actions.size(1)\n",
        "\n",
        "        ins = torch.cat([actions, latents], dim=-1)\n",
        "        outs, _ = self.rnn(ins)\n",
        "        gmm_outs = self.gmm_linear(outs)\n",
        "\n",
        "        stride = self.gaussians * self.latents\n",
        "\n",
        "        mus = gmm_outs[:, :, :stride]\n",
        "        mus = mus.view(seq_len, bs, self.gaussians, self.latents)\n",
        "\n",
        "        sigmas = gmm_outs[:, :, stride:2 * stride]\n",
        "        sigmas = sigmas.view(seq_len, bs, self.gaussians, self.latents)\n",
        "        sigmas = torch.exp(sigmas)\n",
        "\n",
        "        pi = gmm_outs[:, :, 2 * stride: 2 * stride + self.gaussians]\n",
        "        pi = pi.view(seq_len, bs, self.gaussians)\n",
        "        logpi = f.log_softmax(pi, dim=-1)\n",
        "\n",
        "        rs = gmm_outs[:, :, -2]\n",
        "\n",
        "        ds = gmm_outs[:, :, -1]\n",
        "\n",
        "        return mus, sigmas, logpi, rs, ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "370wXnXMutwe"
      },
      "source": [
        "class MDRNNCell(_MDRNNBase):\n",
        "    \"\"\" MDRNN model for one step forward \"\"\"\n",
        "    def __init__(self, latents, actions, hiddens, gaussians):\n",
        "        super(MDRNNCell, self).__init__(latents, actions, hiddens, gaussians)\n",
        "        self.rnn = nn.LSTMCell(latents + actions, hiddens)\n",
        "\n",
        "    def forward(self, action, latent, hidden): # pylint: disable=arguments-differ\n",
        "        \"\"\" ONE STEP forward.\n",
        "        :args actions: (BSIZE, ASIZE) torch tensor\n",
        "        :args latents: (BSIZE, LSIZE) torch tensor\n",
        "        :args hidden: (BSIZE, RSIZE) torch tensor\n",
        "        :returns: mu_nlat, sig_nlat, pi_nlat, r, d, next_hidden, parameters of\n",
        "        the GMM prediction for the next latent, gaussian prediction of the\n",
        "        reward, logit prediction of terminality and next hidden state.\n",
        "            - mu_nlat: (BSIZE, N_GAUSS, LSIZE) torch tensor\n",
        "            - sigma_nlat: (BSIZE, N_GAUSS, LSIZE) torch tensor\n",
        "            - logpi_nlat: (BSIZE, N_GAUSS) torch tensor\n",
        "            - rs: (BSIZE) torch tensor\n",
        "            - ds: (BSIZE) torch tensor\n",
        "        \"\"\"\n",
        "        in_al = torch.cat([action, latent], dim=1)\n",
        "\n",
        "        next_hidden = self.rnn(in_al, hidden)\n",
        "        out_rnn = next_hidden[0]\n",
        "\n",
        "        out_full = self.gmm_linear(out_rnn)\n",
        "\n",
        "        stride = self.gaussians * self.latents\n",
        "\n",
        "        mus = out_full[:, :stride]\n",
        "        mus = mus.view(-1, self.gaussians, self.latents)\n",
        "\n",
        "        sigmas = out_full[:, stride:2 * stride]\n",
        "        sigmas = sigmas.view(-1, self.gaussians, self.latents)\n",
        "        sigmas = torch.exp(sigmas)\n",
        "\n",
        "        pi = out_full[:, 2 * stride:2 * stride + self.gaussians]\n",
        "        pi = pi.view(-1, self.gaussians)\n",
        "        logpi = f.log_softmax(pi, dim=-1)\n",
        "\n",
        "        r = out_full[:, -2]\n",
        "\n",
        "        d = out_full[:, -1]\n",
        "\n",
        "        return mus, sigmas, logpi, r, d, next_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOhe2zr6xm1b"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, device, batch_size=250):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 4, stride=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.mufc = nn.Linear(1024, 32)\n",
        "        self.logvarfc = nn.Linear(1024, 32)\n",
        "        \n",
        "        self.decoder_fc = nn.Linear(32, 1024)\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 128, 5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 6, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 6, stride=2),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        #self.dist = torch.distributions.laplace.Laplace(0, torch.ones([50]))\n",
        "        \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        #noise = torch.randn(self.batch_size, 32).to(self.device)\n",
        "        noise = torch.randn_like(std).to(self.device)\n",
        "        return mu + std * noise # z\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.reshape(-1, 1024)\n",
        "        mu, logvar = self.mufc(x), self.logvarfc(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        z_ = self.decoder_fc(z)\n",
        "        z_ = z_.reshape(-1, 1024, 1, 1)\n",
        "        return self.decoder(z_.float()), mu, logvar\n",
        "    \n",
        "    def get_z(self, x):\n",
        "        with torch.no_grad():\n",
        "            encoded = self.encoder(x).reshape(-1, 1024)\n",
        "            mu, logvar = self.mufc(encoded), self.logvarfc(encoded)\n",
        "            return self.reparameterize(mu, logvar)\n",
        "\n",
        "    def loss_func(self, x, x_prime, mu, logvar):\n",
        "      recon_loss = nn.BCELoss(reduction='sum')\n",
        "      loss = recon_loss(x_prime, x)\n",
        "      loss += -0.5 * torch.sum(1 + logvar - mu.pow(2) - torch.exp(logvar))\n",
        "\n",
        "      return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crw7-gOEx9rn",
        "outputId": "b530f92a-3c82-466b-d8de-7332f8349f38"
      },
      "source": [
        "vae = VAE(device)\n",
        "vae.to(device, dtype=torch.float)\n",
        "vae_path = BASE_PATH + \"vae_original.pt\"\n",
        "vae.load_state_dict(torch.load(vae_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRsFQygUyWrG"
      },
      "source": [
        "class RolloutVaeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dir_path, transform=None):\n",
        "        super(RolloutVaeDataset, self).__init__()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        self.data = []\n",
        "        if (dir_path[-1] != '/'):\n",
        "            dir_path += '/'\n",
        "        for file in os.listdir(dir_path):\n",
        "          file_np = np.load(dir_path + str(file))\n",
        "          imgs = file_np['obs'] # 1000 x 64 x 64 x 3\n",
        "          actions = file_np['action']\n",
        "          for i in range(len(imgs)):\n",
        "            curr_img = imgs[i]\n",
        "            curr_action = actions[i]\n",
        "            self.data.append((curr_img, curr_action))\n",
        "            #self.data.append((np.transpose(curr_img, (2, 0, 1))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # TODO\n",
        "        if (self.transform):\n",
        "          return self.transform(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "        else:\n",
        "          return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3aLa8n5jPgs"
      },
      "source": [
        "\"\"\" Some data loading utilities \"\"\"\n",
        "from bisect import bisect\n",
        "from os import listdir\n",
        "from os.path import join, isdir\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "\n",
        "class _RolloutDataset(torch.utils.data.Dataset): # pylint: disable=too-few-public-methods\n",
        "    def __init__(self, root, transform, buffer_size=200, train=True): # pylint: disable=too-many-arguments\n",
        "        self._transform = transform\n",
        "\n",
        "        self._files = [root + \"/\" + file for file in os.listdir(root)]\n",
        "\n",
        "        # if train:\n",
        "        #     self._files = self._files[:-600]\n",
        "        # else:\n",
        "        #     self._files = self._files[-600:]\n",
        "\n",
        "        self._cum_size = None\n",
        "        self._buffer = None\n",
        "        self._buffer_fnames = None\n",
        "        self._buffer_index = 0\n",
        "        self._buffer_size = buffer_size\n",
        "\n",
        "    def load_next_buffer(self):\n",
        "        \"\"\" Loads next buffer \"\"\"\n",
        "        self._buffer_fnames = self._files[self._buffer_index:self._buffer_index + self._buffer_size]\n",
        "        self._buffer_index += self._buffer_size\n",
        "        self._buffer_index = self._buffer_index % len(self._files)\n",
        "        self._buffer = []\n",
        "        self._cum_size = [0]\n",
        "\n",
        "        # progress bar\n",
        "        pbar = tqdm(total=len(self._buffer_fnames),\n",
        "                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} {postfix}')\n",
        "        pbar.set_description(\"Loading file buffer ...\")\n",
        "\n",
        "        for f in self._buffer_fnames:\n",
        "            with np.load(f) as data:\n",
        "                self._buffer += [{k: np.copy(v) for k, v in data.items()}]\n",
        "                self._cum_size += [self._cum_size[-1] +\n",
        "                                   self._data_per_sequence(data['reward'].shape[0])]\n",
        "            pbar.update(1)\n",
        "        pbar.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        # to have a full sequence, you need self.seq_len + 1 elements, as\n",
        "        # you must produce both an seq_len obs and seq_len next_obs sequences\n",
        "        if not self._cum_size:\n",
        "            self.load_next_buffer()\n",
        "        return self._cum_size[-1]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # binary search through cum_size\n",
        "        file_index = bisect(self._cum_size, i) - 1\n",
        "        seq_index = i - self._cum_size[file_index]\n",
        "        data = self._buffer[file_index]\n",
        "        return self._get_data(data, seq_index)\n",
        "\n",
        "    def _get_data(self, data, seq_index):\n",
        "        pass\n",
        "\n",
        "    def _data_per_sequence(self, data_length):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RolloutSequenceDataset(_RolloutDataset): # pylint: disable=too-few-public-methods\n",
        "    \"\"\" Encapsulates rollouts.\n",
        "    Rollouts should be stored in subdirs of the root directory, in the form of npz files,\n",
        "    each containing a dictionary with the keys:\n",
        "        - observations: (rollout_len, *obs_shape)\n",
        "        - actions: (rollout_len, action_size)\n",
        "        - rewards: (rollout_len,)\n",
        "        - terminals: (rollout_len,), boolean\n",
        "     As the dataset is too big to be entirely stored in rams, only chunks of it\n",
        "     are stored, consisting of a constant number of files (determined by the\n",
        "     buffer_size parameter).  Once built, buffers must be loaded with the\n",
        "     load_next_buffer method.\n",
        "    Data are then provided in the form of tuples (obs, action, reward, terminal, next_obs):\n",
        "    - obs: (seq_len, *obs_shape)\n",
        "    - actions: (seq_len, action_size)\n",
        "    - reward: (seq_len,)\n",
        "    - terminal: (seq_len,) boolean\n",
        "    - next_obs: (seq_len, *obs_shape)\n",
        "    NOTE: seq_len < rollout_len in moste use cases\n",
        "    :args root: root directory of data sequences\n",
        "    :args seq_len: number of timesteps extracted from each rollout\n",
        "    :args transform: transformation of the observations\n",
        "    :args train: if True, train data, else test\n",
        "    \"\"\"\n",
        "    def __init__(self, root, seq_len, transform, buffer_size=200, train=True): # pylint: disable=too-many-arguments\n",
        "        super().__init__(root, transform, buffer_size, train)\n",
        "        self._seq_len = seq_len\n",
        "\n",
        "    def _get_data(self, data, seq_index):\n",
        "        obs_data = data['obs'][seq_index:seq_index + self._seq_len + 1]\n",
        "        obs_data = self._transform(obs_data.astype(np.float32))\n",
        "        obs, next_obs = obs_data[:-1], obs_data[1:]\n",
        "        action = data['action'][seq_index+1:seq_index + self._seq_len + 1]\n",
        "        action = action.astype(np.float32)\n",
        "        reward, terminal = [data[key][seq_index+1:\n",
        "                                      seq_index + self._seq_len + 1].astype(np.float32)\n",
        "                            for key in ('reward', 'done')]\n",
        "        # data is given in the form\n",
        "        # (obs, action, reward, terminal, next_obs)\n",
        "        return obs, action, reward, terminal, next_obs\n",
        "\n",
        "    def _data_per_sequence(self, data_length):\n",
        "        return data_length - self._seq_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnh_Doccy3pt"
      },
      "source": [
        "path = BASE_PATH + \"record/\"\n",
        "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                             torchvision.transforms.Normalize(mean=(0,), std=(1,))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmYDRkeniY6A"
      },
      "source": [
        "LATENT_SIZE=32\n",
        "ACTION_SIZE=3\n",
        "HIDDEN_SIZE=64\n",
        "GAUSSIAN_SIZE=5\n",
        "SEQ_LEN=32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4DYO3RW8eh2",
        "outputId": "8132046a-7b6e-4a3c-c6d8-95ec110408f2"
      },
      "source": [
        "mdrnn = MDRNN(LATENT_SIZE, ACTION_SIZE, HIDDEN_SIZE, GAUSSIAN_SIZE).to(device)\n",
        "optimizer = torch.optim.RMSprop(mdrnn.parameters(), lr=1e-3, alpha=.9)\n",
        "transform = torchvision.transforms.Lambda(lambda x: np.transpose(x, (0, 3, 1, 2)) / 255)\n",
        "mdrnn_train_loader = torch.utils.data.DataLoader(RolloutSequenceDataset(path + \"train\", SEQ_LEN, transform, buffer_size=30), batch_size=256, shuffle=True)\n",
        "mdrnn_test_loader = torch.utils.data.DataLoader(RolloutSequenceDataset(path + \"test\", SEQ_LEN, transform, buffer_size=30), batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading file buffer ...: 100%|██████████| 30/30 \n",
            "Loading file buffer ...: 100%|██████████| 20/20 \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNtYiGHDEhqq"
      },
      "source": [
        "for i, data in enumerate(mdrnn_train_loader):\n",
        "  obs, action, reward, terminal, next_obs = [arr.to(device) for arr in data]\n",
        "  break"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5DkEvisdUXw",
        "outputId": "740e9cd4-9a34-49b6-dd52-6ab5bcb4566e"
      },
      "source": [
        "obs.shape, action.shape, reward.shape, terminal.shape, next_obs.shape"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([256, 32, 3, 64, 64]),\n",
              " torch.Size([256, 32, 3]),\n",
              " torch.Size([256, 32]),\n",
              " torch.Size([256, 32]),\n",
              " torch.Size([256, 32, 3, 64, 64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTfwg9hhuurK"
      },
      "source": [
        "def get_latent_obs(obs):\n",
        "  latent_obs = torch.Tensor().to(device)\n",
        "  for i in obs:\n",
        "    latent_obs = torch.cat((latent_obs, torch.unsqueeze(vae.get_z(i), 0)))\n",
        "  return latent_obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN0qSui9inrU"
      },
      "source": [
        "import tqdm\n",
        "def train(model, device, optimizer, train_loader, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "      obs, action, reward, terminal, next_obs = [arr.to(device) for arr in data]\n",
        "      latent_obs, next_latent_obs = get_latent_obs(obs), get_latent_obs(next_obs)\n",
        "      loss = total_loss(latent_obs, action, reward, terminal, next_latent_obs, model)\n",
        "      losses.append(loss.item())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch_idx % log_interval == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(obs), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, optimizer, test_loader, epoch, log_interval):\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  for batch_idx, data in enumerate(test_loader):\n",
        "    obs, action, reward, terminal, next_obs = [arr.to(device) for arr in data]\n",
        "    latent_obs, next_latent_obs = get_latent_obs(obs), get_latent_obs(next_obs)\n",
        "    loss = total_loss(latent_obs, action, reward, terminal, next_latent_obs, model)\n",
        "    losses.append(loss.item())\n",
        "    if batch_idx % log_interval == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(obs), len(test_loader.dataset),\n",
        "            100. * batch_idx / len(test_loader), loss.item()))\n",
        "  return np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH75Dyq-l4kO",
        "outputId": "038d229b-4f92-4f98-e9b3-78639eadcba0"
      },
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  train_loss = train(mdrnn, device, optimizer, mdrnn_train_loader, epoch, 20)\n",
        "  test_loss = test(mdrnn, device, optimizer, mdrnn_test_loader, epoch, 20)\n",
        "  print(f\"TRAIN LOSS: {train_loss}\")\n",
        "  print(f\"TEST LOSS: {test_loss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/22916 (0%)]\tLoss: 1.351692\n",
            "Train Epoch: 0 [5120/22916 (22%)]\tLoss: 1.260537\n",
            "Train Epoch: 0 [10240/22916 (44%)]\tLoss: 1.199242\n",
            "Train Epoch: 0 [15360/22916 (67%)]\tLoss: 1.142265\n",
            "Train Epoch: 0 [20480/22916 (89%)]\tLoss: 1.112039\n",
            "Train Epoch: 0 [0/15812 (0%)]\tLoss: 1.132878\n",
            "Train Epoch: 0 [5120/15812 (32%)]\tLoss: 1.125570\n",
            "Train Epoch: 0 [10240/15812 (65%)]\tLoss: 1.119184\n",
            "Train Epoch: 0 [15360/15812 (97%)]\tLoss: 1.120739\n",
            "TRAIN LOSS: 1.2080542418691846\n",
            "TEST LOSS: 1.1278328318749704\n",
            "Train Epoch: 1 [0/22916 (0%)]\tLoss: 1.149510\n",
            "Train Epoch: 1 [5120/22916 (22%)]\tLoss: 1.081976\n",
            "Train Epoch: 1 [10240/22916 (44%)]\tLoss: 1.083903\n",
            "Train Epoch: 1 [15360/22916 (67%)]\tLoss: 1.041992\n",
            "Train Epoch: 1 [20480/22916 (89%)]\tLoss: 1.026882\n",
            "Train Epoch: 1 [0/15812 (0%)]\tLoss: 1.030589\n",
            "Train Epoch: 1 [5120/15812 (32%)]\tLoss: 1.077575\n",
            "Train Epoch: 1 [10240/15812 (65%)]\tLoss: 1.034664\n",
            "Train Epoch: 1 [15360/15812 (97%)]\tLoss: 1.071026\n",
            "TRAIN LOSS: 1.0585249556435479\n",
            "TEST LOSS: 1.0352778338616895\n",
            "Train Epoch: 2 [0/22916 (0%)]\tLoss: 1.016863\n",
            "Train Epoch: 2 [5120/22916 (22%)]\tLoss: 1.010561\n",
            "Train Epoch: 2 [10240/22916 (44%)]\tLoss: 1.012708\n",
            "Train Epoch: 2 [15360/22916 (67%)]\tLoss: 0.984780\n",
            "Train Epoch: 2 [20480/22916 (89%)]\tLoss: 0.981338\n",
            "Train Epoch: 2 [0/15812 (0%)]\tLoss: 0.994631\n",
            "Train Epoch: 2 [5120/15812 (32%)]\tLoss: 0.993233\n",
            "Train Epoch: 2 [10240/15812 (65%)]\tLoss: 0.994789\n",
            "Train Epoch: 2 [15360/15812 (97%)]\tLoss: 0.996832\n",
            "TRAIN LOSS: 1.0031214469008976\n",
            "TEST LOSS: 0.9956617672597209\n",
            "Train Epoch: 3 [0/22916 (0%)]\tLoss: 0.984447\n",
            "Train Epoch: 3 [5120/22916 (22%)]\tLoss: 0.983467\n",
            "Train Epoch: 3 [10240/22916 (44%)]\tLoss: 0.987852\n",
            "Train Epoch: 3 [15360/22916 (67%)]\tLoss: 0.959661\n",
            "Train Epoch: 3 [20480/22916 (89%)]\tLoss: 0.964698\n",
            "Train Epoch: 3 [0/15812 (0%)]\tLoss: 0.974597\n",
            "Train Epoch: 3 [5120/15812 (32%)]\tLoss: 0.968700\n",
            "Train Epoch: 3 [10240/15812 (65%)]\tLoss: 0.973157\n",
            "Train Epoch: 3 [15360/15812 (97%)]\tLoss: 0.975637\n",
            "TRAIN LOSS: 0.9790780961513519\n",
            "TEST LOSS: 0.975283436236843\n",
            "Train Epoch: 4 [0/22916 (0%)]\tLoss: 0.976284\n",
            "Train Epoch: 4 [5120/22916 (22%)]\tLoss: 0.964689\n",
            "Train Epoch: 4 [10240/22916 (44%)]\tLoss: 0.999613\n",
            "Train Epoch: 4 [15360/22916 (67%)]\tLoss: 0.973269\n",
            "Train Epoch: 4 [20480/22916 (89%)]\tLoss: 0.950145\n",
            "Train Epoch: 4 [0/15812 (0%)]\tLoss: 0.965636\n",
            "Train Epoch: 4 [5120/15812 (32%)]\tLoss: 0.972227\n",
            "Train Epoch: 4 [10240/15812 (65%)]\tLoss: 0.969740\n",
            "Train Epoch: 4 [15360/15812 (97%)]\tLoss: 0.965909\n",
            "TRAIN LOSS: 0.9662396550178528\n",
            "TEST LOSS: 0.9674212653790751\n",
            "Train Epoch: 5 [0/22916 (0%)]\tLoss: 0.970327\n",
            "Train Epoch: 5 [5120/22916 (22%)]\tLoss: 0.958729\n",
            "Train Epoch: 5 [10240/22916 (44%)]\tLoss: 0.954194\n",
            "Train Epoch: 5 [15360/22916 (67%)]\tLoss: 0.965767\n",
            "Train Epoch: 5 [20480/22916 (89%)]\tLoss: 0.958415\n",
            "Train Epoch: 5 [0/15812 (0%)]\tLoss: 0.963870\n",
            "Train Epoch: 5 [5120/15812 (32%)]\tLoss: 0.990356\n",
            "Train Epoch: 5 [10240/15812 (65%)]\tLoss: 0.968749\n",
            "Train Epoch: 5 [15360/15812 (97%)]\tLoss: 0.965974\n",
            "TRAIN LOSS: 0.9583552281061808\n",
            "TEST LOSS: 0.9616825090300652\n",
            "Train Epoch: 6 [0/22916 (0%)]\tLoss: 0.995123\n",
            "Train Epoch: 6 [5120/22916 (22%)]\tLoss: 0.959217\n",
            "Train Epoch: 6 [10240/22916 (44%)]\tLoss: 0.947954\n",
            "Train Epoch: 6 [15360/22916 (67%)]\tLoss: 0.947218\n",
            "Train Epoch: 6 [20480/22916 (89%)]\tLoss: 0.945035\n",
            "Train Epoch: 6 [0/15812 (0%)]\tLoss: 0.962030\n",
            "Train Epoch: 6 [5120/15812 (32%)]\tLoss: 0.963646\n",
            "Train Epoch: 6 [10240/15812 (65%)]\tLoss: 0.955617\n",
            "Train Epoch: 6 [15360/15812 (97%)]\tLoss: 0.983249\n",
            "TRAIN LOSS: 0.9524621751573351\n",
            "TEST LOSS: 0.9585492293680867\n",
            "Train Epoch: 7 [0/22916 (0%)]\tLoss: 0.948294\n",
            "Train Epoch: 7 [5120/22916 (22%)]\tLoss: 0.947722\n",
            "Train Epoch: 7 [10240/22916 (44%)]\tLoss: 0.947387\n",
            "Train Epoch: 7 [15360/22916 (67%)]\tLoss: 0.943902\n",
            "Train Epoch: 7 [20480/22916 (89%)]\tLoss: 0.942686\n",
            "Train Epoch: 7 [0/15812 (0%)]\tLoss: 0.964229\n",
            "Train Epoch: 7 [5120/15812 (32%)]\tLoss: 0.952711\n",
            "Train Epoch: 7 [10240/15812 (65%)]\tLoss: 0.947628\n",
            "Train Epoch: 7 [15360/15812 (97%)]\tLoss: 0.952806\n",
            "TRAIN LOSS: 0.948262666993671\n",
            "TEST LOSS: 0.9555230871323617\n",
            "Train Epoch: 8 [0/22916 (0%)]\tLoss: 0.951308\n",
            "Train Epoch: 8 [5120/22916 (22%)]\tLoss: 0.935717\n",
            "Train Epoch: 8 [10240/22916 (44%)]\tLoss: 0.935097\n",
            "Train Epoch: 8 [15360/22916 (67%)]\tLoss: 0.944001\n",
            "Train Epoch: 8 [20480/22916 (89%)]\tLoss: 0.932260\n",
            "Train Epoch: 8 [0/15812 (0%)]\tLoss: 0.958222\n",
            "Train Epoch: 8 [5120/15812 (32%)]\tLoss: 0.953450\n",
            "Train Epoch: 8 [10240/15812 (65%)]\tLoss: 0.953715\n",
            "Train Epoch: 8 [15360/15812 (97%)]\tLoss: 0.957354\n",
            "TRAIN LOSS: 0.9444746236006419\n",
            "TEST LOSS: 0.9528036723213811\n",
            "Train Epoch: 9 [0/22916 (0%)]\tLoss: 0.931356\n",
            "Train Epoch: 9 [5120/22916 (22%)]\tLoss: 0.939735\n",
            "Train Epoch: 9 [10240/22916 (44%)]\tLoss: 0.969585\n",
            "Train Epoch: 9 [15360/22916 (67%)]\tLoss: 0.942397\n",
            "Train Epoch: 9 [20480/22916 (89%)]\tLoss: 0.973664\n",
            "Train Epoch: 9 [0/15812 (0%)]\tLoss: 0.937247\n",
            "Train Epoch: 9 [5120/15812 (32%)]\tLoss: 0.954332\n",
            "Train Epoch: 9 [10240/15812 (65%)]\tLoss: 0.948387\n",
            "Train Epoch: 9 [15360/15812 (97%)]\tLoss: 0.944558\n",
            "TRAIN LOSS: 0.9416323277685378\n",
            "TEST LOSS: 0.9499958590153725\n",
            "Train Epoch: 10 [0/22916 (0%)]\tLoss: 0.942459\n",
            "Train Epoch: 10 [5120/22916 (22%)]\tLoss: 0.943087\n",
            "Train Epoch: 10 [10240/22916 (44%)]\tLoss: 0.975192\n",
            "Train Epoch: 10 [15360/22916 (67%)]\tLoss: 0.937840\n",
            "Train Epoch: 10 [20480/22916 (89%)]\tLoss: 0.945417\n",
            "Train Epoch: 10 [0/15812 (0%)]\tLoss: 0.991522\n",
            "Train Epoch: 10 [5120/15812 (32%)]\tLoss: 0.942425\n",
            "Train Epoch: 10 [10240/15812 (65%)]\tLoss: 0.946675\n",
            "Train Epoch: 10 [15360/15812 (97%)]\tLoss: 0.938778\n",
            "TRAIN LOSS: 0.9390058716138204\n",
            "TEST LOSS: 0.949752627841888\n",
            "Train Epoch: 11 [0/22916 (0%)]\tLoss: 0.927627\n",
            "Train Epoch: 11 [5120/22916 (22%)]\tLoss: 0.929419\n",
            "Train Epoch: 11 [10240/22916 (44%)]\tLoss: 0.932919\n",
            "Train Epoch: 11 [15360/22916 (67%)]\tLoss: 0.932655\n",
            "Train Epoch: 11 [20480/22916 (89%)]\tLoss: 0.932768\n",
            "Train Epoch: 11 [0/15812 (0%)]\tLoss: 0.937964\n",
            "Train Epoch: 11 [5120/15812 (32%)]\tLoss: 0.945465\n",
            "Train Epoch: 11 [10240/15812 (65%)]\tLoss: 0.959100\n",
            "Train Epoch: 11 [15360/15812 (97%)]\tLoss: 0.944049\n",
            "TRAIN LOSS: 0.9367363552252451\n",
            "TEST LOSS: 0.9518741148133432\n",
            "Train Epoch: 12 [0/22916 (0%)]\tLoss: 0.926578\n",
            "Train Epoch: 12 [5120/22916 (22%)]\tLoss: 0.932803\n",
            "Train Epoch: 12 [10240/22916 (44%)]\tLoss: 0.930839\n",
            "Train Epoch: 12 [15360/22916 (67%)]\tLoss: 0.932361\n",
            "Train Epoch: 12 [20480/22916 (89%)]\tLoss: 0.930884\n",
            "Train Epoch: 12 [0/15812 (0%)]\tLoss: 0.942600\n",
            "Train Epoch: 12 [5120/15812 (32%)]\tLoss: 0.946038\n",
            "Train Epoch: 12 [10240/15812 (65%)]\tLoss: 0.947727\n",
            "Train Epoch: 12 [15360/15812 (97%)]\tLoss: 0.949113\n",
            "TRAIN LOSS: 0.9348812586731381\n",
            "TEST LOSS: 0.9485522374030082\n",
            "Train Epoch: 13 [0/22916 (0%)]\tLoss: 0.931649\n",
            "Train Epoch: 13 [5120/22916 (22%)]\tLoss: 0.917259\n",
            "Train Epoch: 13 [10240/22916 (44%)]\tLoss: 0.930434\n",
            "Train Epoch: 13 [15360/22916 (67%)]\tLoss: 0.930828\n",
            "Train Epoch: 13 [20480/22916 (89%)]\tLoss: 0.937225\n",
            "Train Epoch: 13 [0/15812 (0%)]\tLoss: 0.945964\n",
            "Train Epoch: 13 [5120/15812 (32%)]\tLoss: 0.927670\n",
            "Train Epoch: 13 [10240/15812 (65%)]\tLoss: 0.938391\n",
            "Train Epoch: 13 [15360/15812 (97%)]\tLoss: 0.934845\n",
            "TRAIN LOSS: 0.9335455709033542\n",
            "TEST LOSS: 0.9467083782918991\n",
            "Train Epoch: 14 [0/22916 (0%)]\tLoss: 0.925332\n",
            "Train Epoch: 14 [5120/22916 (22%)]\tLoss: 0.921434\n",
            "Train Epoch: 14 [10240/22916 (44%)]\tLoss: 0.931369\n",
            "Train Epoch: 14 [15360/22916 (67%)]\tLoss: 0.918698\n",
            "Train Epoch: 14 [20480/22916 (89%)]\tLoss: 0.933987\n",
            "Train Epoch: 14 [0/15812 (0%)]\tLoss: 0.979040\n",
            "Train Epoch: 14 [5120/15812 (32%)]\tLoss: 0.950671\n",
            "Train Epoch: 14 [10240/15812 (65%)]\tLoss: 0.950199\n",
            "Train Epoch: 14 [15360/15812 (97%)]\tLoss: 0.943284\n",
            "TRAIN LOSS: 0.9316480861769783\n",
            "TEST LOSS: 0.9465707636648609\n",
            "Train Epoch: 15 [0/22916 (0%)]\tLoss: 0.926889\n",
            "Train Epoch: 15 [5120/22916 (22%)]\tLoss: 0.926157\n",
            "Train Epoch: 15 [10240/22916 (44%)]\tLoss: 0.928066\n",
            "Train Epoch: 15 [15360/22916 (67%)]\tLoss: 0.917695\n",
            "Train Epoch: 15 [20480/22916 (89%)]\tLoss: 0.932720\n",
            "Train Epoch: 15 [0/15812 (0%)]\tLoss: 0.929801\n",
            "Train Epoch: 15 [5120/15812 (32%)]\tLoss: 0.949472\n",
            "Train Epoch: 15 [10240/15812 (65%)]\tLoss: 0.954840\n",
            "Train Epoch: 15 [15360/15812 (97%)]\tLoss: 0.943775\n",
            "TRAIN LOSS: 0.9303020656108856\n",
            "TEST LOSS: 0.9474343599811677\n",
            "Train Epoch: 16 [0/22916 (0%)]\tLoss: 0.928060\n",
            "Train Epoch: 16 [5120/22916 (22%)]\tLoss: 0.919930\n",
            "Train Epoch: 16 [10240/22916 (44%)]\tLoss: 0.922902\n",
            "Train Epoch: 16 [15360/22916 (67%)]\tLoss: 0.929727\n",
            "Train Epoch: 16 [20480/22916 (89%)]\tLoss: 0.920279\n",
            "Train Epoch: 16 [0/15812 (0%)]\tLoss: 0.938156\n",
            "Train Epoch: 16 [5120/15812 (32%)]\tLoss: 0.936634\n",
            "Train Epoch: 16 [10240/15812 (65%)]\tLoss: 0.941514\n",
            "Train Epoch: 16 [15360/15812 (97%)]\tLoss: 0.944261\n",
            "TRAIN LOSS: 0.9293936358557807\n",
            "TEST LOSS: 0.9425105981288417\n",
            "Train Epoch: 17 [0/22916 (0%)]\tLoss: 0.923790\n",
            "Train Epoch: 17 [5120/22916 (22%)]\tLoss: 0.993351\n",
            "Train Epoch: 17 [10240/22916 (44%)]\tLoss: 0.957635\n",
            "Train Epoch: 17 [15360/22916 (67%)]\tLoss: 0.912657\n",
            "Train Epoch: 17 [20480/22916 (89%)]\tLoss: 0.920882\n",
            "Train Epoch: 17 [0/15812 (0%)]\tLoss: 0.948390\n",
            "Train Epoch: 17 [5120/15812 (32%)]\tLoss: 0.938871\n",
            "Train Epoch: 17 [10240/15812 (65%)]\tLoss: 0.981220\n",
            "Train Epoch: 17 [15360/15812 (97%)]\tLoss: 0.943333\n",
            "TRAIN LOSS: 0.9277558889653947\n",
            "TEST LOSS: 0.9446334396639178\n",
            "Train Epoch: 18 [0/22916 (0%)]\tLoss: 0.920659\n",
            "Train Epoch: 18 [5120/22916 (22%)]\tLoss: 0.919817\n",
            "Train Epoch: 18 [10240/22916 (44%)]\tLoss: 0.915943\n",
            "Train Epoch: 18 [15360/22916 (67%)]\tLoss: 0.926070\n",
            "Train Epoch: 18 [20480/22916 (89%)]\tLoss: 0.922566\n",
            "Train Epoch: 18 [0/15812 (0%)]\tLoss: 0.970547\n",
            "Train Epoch: 18 [5120/15812 (32%)]\tLoss: 0.938643\n",
            "Train Epoch: 18 [10240/15812 (65%)]\tLoss: 0.950085\n",
            "Train Epoch: 18 [15360/15812 (97%)]\tLoss: 0.925825\n",
            "TRAIN LOSS: 0.9272374815411037\n",
            "TEST LOSS: 0.9423146353613946\n",
            "Train Epoch: 19 [0/22916 (0%)]\tLoss: 0.919563\n",
            "Train Epoch: 19 [5120/22916 (22%)]\tLoss: 0.928041\n",
            "Train Epoch: 19 [10240/22916 (44%)]\tLoss: 0.922787\n",
            "Train Epoch: 19 [15360/22916 (67%)]\tLoss: 0.934051\n",
            "Train Epoch: 19 [20480/22916 (89%)]\tLoss: 0.917514\n",
            "Train Epoch: 19 [0/15812 (0%)]\tLoss: 0.931803\n",
            "Train Epoch: 19 [5120/15812 (32%)]\tLoss: 0.982087\n",
            "Train Epoch: 19 [10240/15812 (65%)]\tLoss: 0.936458\n",
            "Train Epoch: 19 [15360/15812 (97%)]\tLoss: 0.934335\n",
            "TRAIN LOSS: 0.9264264073636796\n",
            "TEST LOSS: 0.9427795823543302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se4iZArHEFHm"
      },
      "source": [
        "torch.save(mdrnn.state_dict(), BASE_PATH + \"mdrnn.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}